{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEf0c6yOq9Rh"
   },
   "source": [
    "## Context\n",
    "\n",
    "Banks incur significant losses due to default in loans. This has led to a tightening up of loan underwriting and has increased loan rejection rates. The need for a better credit risk scoring model is also raised by banks.\n",
    "\n",
    "The CNK bank has collected customer data for the past few years and wants to build a model to predict if a customer coming to purchase a loan is a good customer (will not default) or a bad customer (will default).\n",
    "\n",
    "\n",
    "## Data Dictionary\n",
    "\n",
    "- month - the month of purchase\n",
    "- credit_amount - amount for which loan is requested\n",
    "- credit_term - for how long customer wants a loan\n",
    "- age - age of the customer\n",
    "- sex - gender of the customer\n",
    "- education - education level of customer\n",
    "- product_type - for purchasing what type of product does the customer need a loan (0, 1, 2, 3, 4)\n",
    "- having_children_flg - if the customer has children or not\n",
    "- region - customer region category(0, 1, 2)\n",
    "- income - income of the customer\n",
    "- family_status - another, married, unmarried\n",
    "- phone_operator - mobile operator category(0, 1, 2, 3)\n",
    "- is_client - if the customer wanting to purchase a loan is our client or not\n",
    "- target - 1-bad customer, 0-good customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pxzWGKaX1AN"
   },
   "source": [
    "## Imblearn installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjZwsAtZX1AN",
    "outputId": "6d46aa32-e76f-486d-b48a-79da98c65dd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (from imblearn) (0.8.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.20.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (2.1.0)\n",
      "Requirement already satisfied: imbalanced-learn in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (0.8.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.20.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/bob/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Imblearn libary is used to handle imbalanced data\n",
    "\n",
    "# Jupyter notebook\n",
    "!pip install imblearn --user\n",
    "\n",
    "!pip install imbalanced-learn --user\n",
    "\n",
    "# Anaconda prompt\n",
    "#!pip install -U imbalanced-learn\n",
    "\n",
    "#conda install -c conda-forge imbalanced-learn\n",
    "\n",
    "# Restart the kernel after successful installation of the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnxGmBkvq9Rj"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UK2k3Mg-q9Rj"
   },
   "outputs": [],
   "source": [
    "# To help with reading and manipulation of data\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "\n",
    "# To help with data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# To impute missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# To do one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# To build a decision tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# To get different performance metrics\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "\n",
    "# To undersample and oversample the data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# To suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhNJ3tawq9Rk"
   },
   "source": [
    "## Load and view dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMpzVuhigC-F",
    "outputId": "3703cc31-3c27-433c-e3bd-621446e386d4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-fedd4ac02989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/gdrive/MyDrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/MyDrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jKK-0Zx2gxE"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Loanclients.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRePnbBq5cJd"
   },
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "Fu9Ni_Kk5cJe",
    "outputId": "07816722-8ae2-420f-8378-a1a0d3997d39"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XlCskF3Fq9Rl",
    "outputId": "f0a78b06-9c30-4e71-b498-76230327a1f0"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "XyD9Z5N40FMQ",
    "outputId": "02ff842f-b8d1-4429-cae6-473d67b1557d"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evq2UOZx2gxS"
   },
   "source": [
    "#### Let's look at the pairplot of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TNjI8nxn2gxT",
    "outputId": "b14c86a1-be4d-4b95-efb9-ca80793b24cb"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(data, hue = 'target' , diag_kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XbFryla5cJe",
    "outputId": "ee0f767d-d250-4fb5-e7b4-b73f501efe61"
   },
   "outputs": [],
   "source": [
    "# checking missing values in the data\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQHkb8_W5cJf"
   },
   "source": [
    "**The income variable has some missing values, we will impute them later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-g1iA3vkhIzq"
   },
   "outputs": [],
   "source": [
    "data[\"phone_operator\"] = data[\"phone_operator\"].astype(\"category\")\n",
    "data[\"product_type\"] = data[\"product_type\"].astype(\"category\")\n",
    "data[\"education\"] = data[\"education\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLYE2G08cF00"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIQWS7KuhIzr",
    "outputId": "564ea143-55b4-43ec-9026-6e285540cd6a"
   },
   "outputs": [],
   "source": [
    "# checking the distribution of the target variable\n",
    "data[\"target\"].value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZFkLVZphIzr"
   },
   "source": [
    "### Splitting the data into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82OTq1bvhIzs"
   },
   "outputs": [],
   "source": [
    "# separating the independent and dependent variables\n",
    "X = data.drop([\"target\"], axis=1)\n",
    "y = data[\"target\"]\n",
    "\n",
    "# creating dummy variables\n",
    "X = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJM5CvFOhIzs",
    "outputId": "0b8b72fc-9348-4299-dee5-ceae6c50d784"
   },
   "outputs": [],
   "source": [
    "# Splitting data into training, validation and test set:\n",
    "\n",
    "# first we split data into 2 parts, say temporary and test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "# then we split the temporary set into train and validation\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.4, random_state=0, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oy7LQSmJ5cJg"
   },
   "outputs": [],
   "source": [
    "# Let's impute the missing values\n",
    "imp_median = SimpleImputer(missing_values=np.nan, strategy=\"median\")\n",
    "\n",
    "# fit the imputer on train data and transform the train data\n",
    "X_train[\"income\"] = imp_median.fit_transform(X_train[[\"income\"]])\n",
    "\n",
    "# transform the validation and test data using the imputer fit on train data\n",
    "X_val[\"income\"] = imp_median.transform(X_val[[\"income\"]])\n",
    "X_test[\"income\"] = imp_median.transform(X_test[[\"income\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCOMaaMGhIzs",
    "outputId": "a9956595-8f55-4c4c-e79b-4b109bf24ec6"
   },
   "outputs": [],
   "source": [
    "# Checking class balance for whole data, train set, validation set, and test set\n",
    "\n",
    "print(\"Target value ratio in y\")\n",
    "print(y.value_counts(1))\n",
    "print(\"*\" * 80)\n",
    "print(\"Target value ratio in y_train\")\n",
    "print(y_train.value_counts(1))\n",
    "print(\"*\" * 80)\n",
    "print(\"Target value ratio in y_val\")\n",
    "print(y_val.value_counts(1))\n",
    "print(\"*\" * 80)\n",
    "print(\"Target value ratio in y_test\")\n",
    "print(y_test.value_counts(1))\n",
    "print(\"*\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URYQOmrchIzx"
   },
   "source": [
    "## Model evaluation criterion\n",
    "\n",
    "\n",
    "**What does a bank want?**\n",
    "* A bank wants to minimize the loss - it can face 2 types of losses here: \n",
    "   * Whenever a bank lends money to a customer, they don't return it.\n",
    "   * A bank doesn't lend money to a customer thinking a customer will default but in reality, the customer won't - opportunity loss.\n",
    "\n",
    "**Which loss is greater ?**\n",
    "* Lending to a customer who wouldn't be able to pay back.\n",
    "\n",
    "**Since we want to reduce loan defaults we should use Recall as a metric of model evaluation instead of accuracy.**\n",
    "\n",
    "* Recall - It gives the ratio of True positives to Actual positives, so high Recall implies low false negatives, i.e. low chances of predicting a bad customer as a good customer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PSLBiZFX1AU"
   },
   "source": [
    "## Let's train a decision tree and check it's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UsX5aVzX1AV",
    "outputId": "8aa2e4b7-ff04-4eee-ffb3-41d8ceb27ac4"
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(random_state=1, max_depth=4)\n",
    "\n",
    "# training the decision tree model with oversampled training set\n",
    "dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J4ZXnK2HxpRi",
    "outputId": "42895791-3010-443e-d0ba-14716a10f414"
   },
   "outputs": [],
   "source": [
    "print(pd.DataFrame(dtree.feature_importances_, columns = [\"imp\"], index = X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zv9gAQu65cJn"
   },
   "source": [
    "#### We have trained the model, Let's check the performance on the oversampled train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syEfgbQDX1AV"
   },
   "outputs": [],
   "source": [
    "# Predicting the target for train and validation set\n",
    "pred_train = dtree.predict(X_train)\n",
    "pred_val = dtree.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-v72nSrhIz7",
    "outputId": "c871ae6f-544f-4d3f-c145-63c34be3bac9"
   },
   "outputs": [],
   "source": [
    "# Checking recall score on oversampled train and validation set\n",
    "print(recall_score(y_train, pred_train))\n",
    "print(recall_score(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4ekCtbD2gyC",
    "outputId": "0d37dcf3-c5e7-4520-b089-786c2619f7f5"
   },
   "outputs": [],
   "source": [
    "# Checking accuracy score on oversampled train and validation set\n",
    "print(accuracy_score(y_train, pred_train))\n",
    "print(accuracy_score(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxkIkkD1X1AW"
   },
   "source": [
    "- Accuracy of the model is good but the metric of interest in Recall here.\n",
    "- The model's recall score is poor for both train and validation sets.\n",
    "\n",
    "- Let's try oversampling and undersampling techniques to see if recall improves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-e7SzU6q9Rr"
   },
   "source": [
    "# Oversampling and Undersampling the train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xda_Pbcq9Rx"
   },
   "source": [
    "## Oversampling train data using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kOj8HkH3mQsQ"
   },
   "outputs": [],
   "source": [
    "# Fit SMOTE on train data(Synthetic Minority Oversampling Technique)\n",
    "sm = SMOTE(sampling_strategy=0.4, k_neighbors=5, random_state=1)\n",
    "X_train_over, y_train_over = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXCXOhX6q9Ry",
    "outputId": "3fbc9542-a0c8-425e-c044-67c5b95e3fff"
   },
   "outputs": [],
   "source": [
    "print(\"Before OverSampling, count of label '1': {}\".format(sum(y_train == 1)))\n",
    "print(\"Before OverSampling, count of label '0': {} \\n\".format(sum(y_train == 0)))\n",
    "\n",
    "print(\"After OverSampling, count of label '1': {}\".format(sum(y_train_over == 1)))\n",
    "print(\"After OverSampling, count of label '0': {} \\n\".format(sum(y_train_over == 0)))\n",
    "\n",
    "print(\"After OverSampling, the shape of train_X: {}\".format(X_train_over.shape))\n",
    "print(\"After OverSampling, the shape of train_y: {} \\n\".format(y_train_over.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S88tVqb05cJm"
   },
   "source": [
    "#### Let's train a decision tree classifier using the oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gc3xnh_cq9Ry",
    "outputId": "564e31db-6fe8-499a-e5de-e5d924663f0e"
   },
   "outputs": [],
   "source": [
    "dtree1 = DecisionTreeClassifier(random_state=1, max_depth=4)\n",
    "\n",
    "# training the decision tree model with oversampled training set\n",
    "dtree1.fit(X_train_over, y_train_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SuoqP-epX1AW"
   },
   "source": [
    "#### We have trained the model, Let's check the performance on the oversampled train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWtmtNUzq9Ry"
   },
   "outputs": [],
   "source": [
    "# Predicting the target for train and validation set\n",
    "pred_train = dtree1.predict(X_train_over)\n",
    "pred_val = dtree1.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sfX98IncX1AX",
    "outputId": "4e3f64b4-ff58-4dda-92f3-ad6ea1baea52"
   },
   "outputs": [],
   "source": [
    "# Checking recall score on oversampled train and validation set\n",
    "print(recall_score(y_train_over, pred_train))\n",
    "print(recall_score(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLJcrAz6X1AX",
    "outputId": "13580f28-8d1d-46de-c235-7fc349e5be72"
   },
   "outputs": [],
   "source": [
    "# Checking accuracy score on oversampled train and validation set\n",
    "print(accuracy_score(y_train_over, pred_train))\n",
    "print(accuracy_score(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nv5U4Oy45cJn"
   },
   "source": [
    "#### After checking the performance, let's look at the confusion matrices of the train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "avyMAAI9q9Ry",
    "outputId": "b2f1f35f-1213-4b41-abf5-6eb37c0e272b"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for oversampled train data\n",
    "cm = confusion_matrix(y_train_over, pred_train)\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"g\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Actual Values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "gtZg-CR3q9Rz",
    "outputId": "10a46a09-1e50-4ad1-d0f1-c0eb9194c91b"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for validation data\n",
    "cm = confusion_matrix(y_val, pred_val)\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"g\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Actual Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlAL45RRyd9m"
   },
   "source": [
    "* We can see that the model with oversampled data has improved the train recall from 57% to 92% and 50% to 84% on the validation data\n",
    "* There is a little overfitting in the results which can be handled separately by using regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXdy3Agcq9Rr"
   },
   "source": [
    "## Undersampling train data using Random Undersampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjRrtC9vq9Rw"
   },
   "outputs": [],
   "source": [
    "# fit random under sampler on the train data\n",
    "rus = RandomUnderSampler(random_state=1, sampling_strategy = 1)\n",
    "X_train_un, y_train_un = rus.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vruC7P9-q9Rw",
    "outputId": "f4612bc2-4727-4e64-ab95-e67d67561ccd"
   },
   "outputs": [],
   "source": [
    "print(\"Before Under Sampling, count of label '1': {}\".format(sum(y_train == 1)))\n",
    "print(\"Before Under Sampling, count of label '0': {} \\n\".format(sum(y_train == 0)))\n",
    "\n",
    "print(\"After Under Sampling, count of label '1': {}\".format(sum(y_train_un == 1)))\n",
    "print(\"After Under Sampling, count of label '0': {} \\n\".format(sum(y_train_un == 0)))\n",
    "\n",
    "print(\"After Under Sampling, the shape of train_X: {}\".format(X_train_un.shape))\n",
    "print(\"After Under Sampling, the shape of train_y: {} \\n\".format(y_train_un.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8dXCQwo5cJr"
   },
   "source": [
    "#### Let's train a decision tree classifier using the undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZPiKv_gq9Rw",
    "outputId": "241590c0-2c2b-4ed5-daff-f84383d61ffd"
   },
   "outputs": [],
   "source": [
    "dtree2 = DecisionTreeClassifier(random_state=1, max_depth=4)\n",
    "\n",
    "# training the decision tree model with oversampled training set\n",
    "dtree2.fit(X_train_un, y_train_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxkTeo_95cJs"
   },
   "source": [
    "#### We have trained the model, let's check the performance on undersampled train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Itx88qHkq9Rx"
   },
   "outputs": [],
   "source": [
    "# Predicting the target for train and validation set\n",
    "pred_train = dtree2.predict(X_train_un)\n",
    "pred_val = dtree2.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ISt69RJLhIz5",
    "outputId": "d763dcd6-16d9-4dae-ba18-ba6a61d34fed"
   },
   "outputs": [],
   "source": [
    "# Checking recall score on oversampled train and validation set\n",
    "print(recall_score(y_train_un, pred_train))\n",
    "print(recall_score(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWzkXgpB2gyF",
    "outputId": "9a33566c-4601-492c-8d5f-ae34001bfd16"
   },
   "outputs": [],
   "source": [
    "# Checking accuracy score on undersampled train and validation set\n",
    "print(accuracy_score(y_train_un, pred_train))\n",
    "print(accuracy_score(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPjPDgf45cJs"
   },
   "source": [
    "#### After checking the performance, let's look at the confusion matrices of the train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "BtixB2Tlq9Rx",
    "outputId": "42b868ed-c4ac-4ac0-f854-0d2fabc81864"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for undersampled train data\n",
    "cm = confusion_matrix(y_train_un, pred_train)\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"g\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Actual Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "a-yfKTM5hIz6",
    "outputId": "87bd19c5-7cb6-4609-983b-75a1ddae6317"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for validation data\n",
    "cm = confusion_matrix(y_val, pred_val)\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"g\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Actual Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omMizSikq9Rz"
   },
   "source": [
    "- We can see that the model with undersampled data has improved the train recall from 57% to 88%  and 50% to 84% on the validation data, but the accuracy decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1XG8GCV5cJt"
   },
   "source": [
    "#### We can see that the model trained with undersampled data has better performance, so let's now check the performance of dtree2 using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "_CM-996phIz8",
    "outputId": "fde6baa2-3674-4a5f-ace3-8ab41f5bb743"
   },
   "outputs": [],
   "source": [
    "# Now we have identified the best model, let's check its performance on test set\n",
    "print(recall_score(y_test, dtree2.predict(X_test)))\n",
    "\n",
    "cm = confusion_matrix(y_test, dtree2.predict(X_test))\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"g\")\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Actual Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asMB7n075cJt"
   },
   "source": [
    "* The recall score for the best model on the test data is ~90%\n",
    "\n",
    "**Further performance can be improved by using hyperparameter tuning**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Oversampling and undersampling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
